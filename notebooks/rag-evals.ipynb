{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import nest_asyncio\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "\n",
    "# RAG\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Response\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    RetrieverEvaluator,\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    "    DatasetGenerator\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "#sys.path.append(os.path.join('/Users/juandiegogallegoquiceno/Desktop/pinacle/secrets'))\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# import secrets\n",
    "#from hf_secrets import api_token as hf_token\n",
    "#from openai_secrets import api_key as oai_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using dotenv\n",
    "secrets = dotenv_values(dotenv_path=\"/home/paperspace/Desktop/secrets/.env\")\n",
    "\n",
    "OPENAI_API_KEY = secrets['OPENAI_API_KEY']\n",
    "HF_TOKEN = secrets['HF_API_TOKEN']\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#HF_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "# assign as environ vars\n",
    "#os.environ['OPENAI_API_KEY'] = oai_token\n",
    "#os.environ['HF_TOKEN'] = hf_token\n",
    "#\n",
    "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "#HF_TOKEN = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader('../data')\n",
    "documents = reader.load_data('Final Policy document_LICs New Jeevan Shanti_V05_logo.pdf')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embed model\n",
    "hf_embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_5 = OpenAI(model='gpt-3.5-turbo', temperature=0.1)\n",
    "llama2 = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-2-7b-hf\",\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-hf\",\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to get metric result from batch eval runner\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[f'{key}']\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        if result.passing:\n",
    "            correct += 1\n",
    "    score = correct / len(results)\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data from document\n",
    "num_eval_nodes = 10\n",
    "data_generator = DatasetGenerator.from_documents(documents, llm=llama2)\n",
    "eval_dataset = data_generator.generate_dataset_from_nodes(num=num_eval_nodes)\n",
    "#eval_dataset = data_generator.agenerate_dataset_from_nodes(num=num_eval_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get eval questions and eval answers\n",
    "eval_questions = [ex[0] for ex in eval_dataset.qr_pairs]\n",
    "eval_answers = [ex[1] for ex in eval_dataset.qr_pairs]\n",
    "\n",
    "print(len(eval_questions))\n",
    "print(eval_questions[0], eval_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and eval query example\n",
    "eval_query = eval_questions[0]\n",
    "#### Vector store\n",
    "# with Hugginface Embedding BAAI/bge-small-en-v1.5\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=hf_embed_model, llm=llama2)\n",
    "# query engine to generate response\n",
    "query_engine = index.as_query_engine()\n",
    "# define retriever as well\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "# get nodes\n",
    "nodes = retriever.retrieve(eval_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a runner evaluation instance\n",
    "runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": FaithfulnessEvaluator(llm=llama2),\n",
    "        \"relevancy\": RelevancyEvaluator(llm=llama2),\n",
    "        \"correctness\": CorrectnessEvaluator(llm=llama2)\n",
    "    },\n",
    "    workers=8\n",
    ")\n",
    "# evaluate\n",
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine,\n",
    "    queries=eval_questions,\n",
    "    reference=eval_answers\n",
    ")\n",
    "# see results\n",
    "for key in [\"faithfulness\", \"relevancy\", \"correctness\"]:\n",
    "    get_eval_results(key, eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data from document\n",
    "num_eval_nodes = 10\n",
    "data_generator = DatasetGenerator.from_documents(documents, llm=gpt3_5)\n",
    "eval_dataset = data_generator.generate_dataset_from_nodes(num=num_eval_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get eval questions and eval answers\n",
    "eval_questions = [ex[0] for ex in eval_dataset.qr_pairs]\n",
    "eval_answers = [ex[1] for ex in eval_dataset.qr_pairs]\n",
    "\n",
    "print(len(eval_questions))\n",
    "print(eval_questions[0], eval_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and eval query example\n",
    "eval_query = eval_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Hugginface Embedding BAAI/bge-small-en-v1.5\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=hf_embed_model, llm=gpt3_5)\n",
    "# query engine to generate response\n",
    "query_engine = index.as_query_engine()\n",
    "# define retriever as well\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "# get nodes\n",
    "nodes = retriever.retrieve(eval_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a runner evaluation instance\n",
    "runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": FaithfulnessEvaluator(llm=gpt3_5),\n",
    "        \"relevancy\": RelevancyEvaluator(llm=gpt3_5),\n",
    "        \"correctness\": CorrectnessEvaluator(llm=gpt3_5)\n",
    "    },\n",
    "    workers=8\n",
    ")\n",
    "# evaluate\n",
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine,\n",
    "    queries=eval_questions,\n",
    "    reference=eval_answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see results\n",
    "for key in [\"faithfulness\", \"relevancy\", \"correctness\"]:\n",
    "    get_eval_results(key, eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
